"""
í‚¤ì›Œë“œ ê¸°ë°˜ í˜¸ì¬ ë‰´ìŠ¤ í•„í„°ë§ ëª¨ë“ˆ
"""
import json
import re
from typing import List, Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KeywordFilter:
    """í˜¸ì¬ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°"""
    
    def __init__(self, keywords_file: str, exclude_keywords: List[str] = None):
        """
        Args:
            keywords_file: í‚¤ì›Œë“œ JSON íŒŒì¼ ê²½ë¡œ
            exclude_keywords: ì œì™¸í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        with open(keywords_file, 'r', encoding='utf-8') as f:
            self.keywords_data = json.load(f)
        
        self.exclude_keywords = exclude_keywords or []
        
    def calculate_score(self, text: str) -> Tuple[float, List[str]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        
        Returns:
            (ì ìˆ˜, ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
        """
        text_lower = text.lower()
        total_score = 0.0
        matched_keywords = []
        
        # ê° ìš°ì„ ìˆœìœ„ë³„ í‚¤ì›Œë“œ ê²€ì‚¬
        for priority, data in self.keywords_data.items():
            keywords = data['keywords']
            score_weight = data['score']
            
            for keyword in keywords:
                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ê²€ìƒ‰
                if keyword.lower() in text_lower:
                    total_score += score_weight
                    matched_keywords.append(keyword)
        
        return total_score, matched_keywords
    
    def is_excluded(self, text: str) -> bool:
        """ì œì™¸ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""
        text_lower = text.lower()
        for exclude_kw in self.exclude_keywords:
            if exclude_kw.lower() in text_lower:
                return True
        return False
    
    def filter_news(self, news_list: List[Dict], min_score: float = 3.0) -> List[Dict]:
        """
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
        
        Args:
            news_list: ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
            min_score: ìµœì†Œ ì ìˆ˜ ê¸°ì¤€
            
        Returns:
            í•„í„°ë§ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë†’ì€ ìˆœ)
        """
        filtered_news = []
        
        for news in news_list:
            # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            combined_text = f"{news['title']} {news.get('summary', '')}"
            if self.is_excluded(combined_text):
                continue
            
            # ì ìˆ˜ ê³„ì‚°
            score, keywords = self.calculate_score(combined_text)
            
            if score >= min_score:
                news['score'] = round(score, 1)
                news['matched_keywords'] = keywords
                filtered_news.append(news)
        
        # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_news.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"ğŸ” í•„í„°ë§ ê²°ê³¼: {len(news_list)}ê±´ â†’ {len(filtered_news)}ê±´ (ê¸°ì¤€: {min_score}ì )")
        
        return filtered_news


class DeduplicateFilter:
    """ì¤‘ë³µ ì œê±° í•„í„°"""
    
    def __init__(self, cache_file: str):
        """
        Args:
            cache_file: ìºì‹œ íŒŒì¼ ê²½ë¡œ (JSON)
        """
        self.cache_file = cache_file
        self.seen_urls = self._load_cache()
    
    def _load_cache(self) -> set:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('urls', []))
        except FileNotFoundError:
            return set()
    
    def _save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump({'urls': list(self.seen_urls)}, f, ensure_ascii=False, indent=2)
    
    def remove_duplicates(self, news_list: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±°"""
        unique_news = []
        new_urls = []
        
        for news in news_list:
            url = news['link']
            if url not in self.seen_urls:
                unique_news.append(news)
                self.seen_urls.add(url)
                new_urls.append(url)
        
        # ìºì‹œê°€ ë„ˆë¬´ ì»¤ì§€ë©´ ìµœê·¼ 10000ê°œë§Œ ìœ ì§€
        if len(self.seen_urls) > 10000:
            self.seen_urls = set(list(self.seen_urls)[-10000:])
        
        self._save_cache()
        
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(news_list)}ê±´ â†’ {len(unique_news)}ê±´ (ì‹ ê·œ)")
        
        return unique_news


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    import os
    
    # í‚¤ì›Œë“œ í•„í„° í…ŒìŠ¤íŠ¸
    keywords_file = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'keywords.json')
    filter = KeywordFilter(keywords_file)
    
    test_news = [
        {
            'title': 'ì‚¼ì„±ì „ì, ë°˜ë„ì²´ ë¶€ë¬¸ ì‹¤ì  í˜¸ì¡°ë¡œ ì˜ì—…ì´ìµ ê¸‰ì¦',
            'summary': '3ë¶„ê¸° ì˜ì—…ì´ìµ ì „ë…„ ëŒ€ë¹„ 50% ì¦ê°€',
            'link': 'http://test1.com'
        },
        {
            'title': 'LGì „ì ì‹ ì œí’ˆ ì¶œì‹œ',
            'summary': 'ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸í° ë¼ì¸ì—… ê³µê°œ',
            'link': 'http://test2.com'
        },
        {
            'title': 'ì¼ë°˜ ë‰´ìŠ¤',
            'summary': 'íŠ¹ë³„í•œ ë‚´ìš© ì—†ìŒ',
            'link': 'http://test3.com'
        }
    ]
    
    filtered = filter.filter_news(test_news, min_score=3.0)
    
    print(f"\ní•„í„°ë§ ê²°ê³¼: {len(filtered)}ê±´")
    for news in filtered:
        print(f"\nì œëª©: {news['title']}")
        print(f"ì ìˆ˜: {news['score']}")
        print(f"í‚¤ì›Œë“œ: {', '.join(news['matched_keywords'])}")

