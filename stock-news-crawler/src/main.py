"""
ì£¼ì‹ í˜¸ì¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""
import os
import sys
import yaml
import json
import logging
import time
from datetime import datetime
import schedule

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.collectors import run_collector
from src.filters import KeywordFilter, DeduplicateFilter
from src.generator import HTMLGenerator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('crawler.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class StockNewsCrawler:
    """ì£¼ì‹ í˜¸ì¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (config.yaml)
        """
        # ì„¤ì • ë¡œë“œ
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        self.base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # ê²½ë¡œ ì„¤ì •
        keywords_file = os.path.join(self.base_dir, 'data', 'keywords.json')
        cache_file = os.path.join(self.base_dir, self.config['cache']['file'])
        self.filtered_news_file = os.path.join(self.base_dir, 'data', 'filtered_news.json')
        self.run_history_file = os.path.join(self.base_dir, 'data', 'run_history.json')
        template_path = os.path.join(self.base_dir, 'templates', 'news_template.html')
        output_dir = os.path.join(self.base_dir, self.config['output']['directory'])
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.keyword_filter = KeywordFilter(
            keywords_file,
            self.config['filtering']['exclude_keywords']
        )
        self.dedupe_filter = DeduplicateFilter(cache_file)
        self.html_generator = HTMLGenerator(template_path, output_dir)
    
    def _load_filtered_news(self):
        """ì €ì¥ëœ í•„í„°ë§ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            with open(self.filtered_news_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return []
    
    def _save_filtered_news(self, news_list):
        """í•„í„°ë§ëœ ë‰´ìŠ¤ ì €ì¥ (ëˆ„ì )"""
        with open(self.filtered_news_file, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
    
    def _log_run_history(self, collected_count, new_count, filtered_count, total_count, elapsed_time):
        """ì‹¤í–‰ ì´ë ¥ ê¸°ë¡"""
        try:
            with open(self.run_history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except FileNotFoundError:
            history = []
        
        history.append({
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'collected': collected_count,
            'new': new_count,
            'filtered_new': filtered_count,
            'total_accumulated': total_count,
            'elapsed_seconds': round(elapsed_time, 2)
        })
        
        # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
        if len(history) > 1000:
            history = history[-1000:]
        
        with open(self.run_history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def run_once(self):
        """1íšŒ ì‹¤í–‰"""
        try:
            start_time = time.time()
            logger.info("=" * 60)
            logger.info("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            logger.info("=" * 60)
            
            # 1. RSS ìˆ˜ì§‘
            collected_news = run_collector(
                self.config['rss_sources'],
                self.config['crawler']['timeout']
            )
            collected_count = len(collected_news)
            
            if not collected_news:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                elapsed = time.time() - start_time
                self._log_run_history(0, 0, 0, len(self._load_filtered_news()), elapsed)
                return
            
            # 2. ì¤‘ë³µ ì œê±° (ì‹ ê·œ ë‰´ìŠ¤ë§Œ ì¶”ì¶œ)
            new_news = self.dedupe_filter.remove_duplicates(collected_news)
            new_count = len(new_news)
            
            # 3. ì‹ ê·œ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ í•„í„°ë§
            filtered_new_count = 0
            filtered_new = []
            
            if new_news:
                filtered_new = self.keyword_filter.filter_news(
                    new_news,
                    self.config['filtering']['min_score']
                )
                filtered_new_count = len(filtered_new)
            else:
                logger.info("â„¹ï¸ ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì¤‘ë³µ)")
            
            # 4. ê¸°ì¡´ ë‰´ìŠ¤ ë¶ˆëŸ¬ì™€ì„œ ëˆ„ì 
            existing_news = self._load_filtered_news()
            
            if filtered_new:
                # ì‹ ê·œ í•„í„°ë§ëœ ë‰´ìŠ¤ë¥¼ ë§¨ ì•ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
                all_news = filtered_new + existing_news
                
                # ìµœëŒ€ ê°œìˆ˜ ì œí•œ (ì„¤ì • ê°€ëŠ¥)
                max_news = self.config.get('filtering', {}).get('max_accumulated', 500)
                all_news = all_news[:max_news]
                
                # ì €ì¥
                self._save_filtered_news(all_news)
                
                logger.info(f"ğŸ“Š ì‹ ê·œ í˜¸ì¬ ë‰´ìŠ¤: {filtered_new_count}ê±´ ì¶”ê°€")
                logger.info(f"ğŸ“š ëˆ„ì  í˜¸ì¬ ë‰´ìŠ¤: {len(all_news)}ê±´")
            else:
                if new_news:
                    logger.info("â„¹ï¸ ì‹ ê·œ ë‰´ìŠ¤ ì¤‘ í•„í„°ë§ ê¸°ì¤€ì„ í†µê³¼í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                all_news = existing_news
                logger.info(f"ğŸ“š í˜„ì¬ ëˆ„ì  í˜¸ì¬ ë‰´ìŠ¤: {len(existing_news)}ê±´")
            
            # 5. HTML ìƒì„± (ëˆ„ì  ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ í•­ìƒ ìƒì„±)
            if all_news:
                output_path = self.html_generator.generate_report(all_news)
                logger.info(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")
            else:
                logger.info("â„¹ï¸ ì•„ì§ í•„í„°ë§ëœ í˜¸ì¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì‹¤í–‰ ì‹œê°„
            elapsed = time.time() - start_time
            logger.info(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            
            # ì‹¤í–‰ ì´ë ¥ ê¸°ë¡
            self._log_run_history(collected_count, new_count, filtered_new_count, len(all_news), elapsed)
            
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    
    def run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (5ë¶„ë§ˆë‹¤)"""
        interval_seconds = self.config['crawler']['interval']
        interval_minutes = interval_seconds / 60
        
        logger.info("=" * 60)
        logger.info(f"ğŸ¤– ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘: {interval_minutes}ë¶„ë§ˆë‹¤ ì‹¤í–‰")
        logger.info("=" * 60)
        
        # ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
        self.run_once()
        
        # ìŠ¤ì¼€ì¤„ ë“±ë¡
        schedule.every(interval_seconds).seconds.do(self.run_once)
        
        # ë¬´í•œ ë£¨í”„
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì£¼ì‹ í˜¸ì¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬')
    parser.add_argument(
        '--mode',
        choices=['once', 'schedule'],
        default='schedule',
        help='ì‹¤í–‰ ëª¨ë“œ: once (1íšŒ ì‹¤í–‰) ë˜ëŠ” schedule (ìŠ¤ì¼€ì¤„ëŸ¬)'
    )
    parser.add_argument(
        '--config',
        default='config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    if not os.path.isabs(args.config):
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(base_dir, args.config)
    else:
        config_path = args.config
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = StockNewsCrawler(config_path)
    
    # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¼ ì‹¤í–‰
    if args.mode == 'once':
        crawler.run_once()
    else:
        crawler.run_scheduler()


if __name__ == "__main__":
    main()

