"""
RSS í”¼ë“œ ìˆ˜ì§‘ ëª¨ë“ˆ (ë¹„ë™ê¸° ì²˜ë¦¬)
"""
import asyncio
import aiohttp
import feedparser
from datetime import datetime, timezone, timedelta
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RSSCollector:
    """ë¹„ë™ê¸° RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, rss_sources: List[Dict], timeout: int = 10):
        """
        Args:
            rss_sources: RSS ì†ŒìŠ¤ ëª©ë¡ (config.yamlì—ì„œ ë¡œë“œ)
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.rss_sources = [source for source in rss_sources if source.get('enabled', True)]
        self.timeout = timeout
        
    async def fetch_feed(self, session: aiohttp.ClientSession, source: Dict) -> List[Dict]:
        """ë‹¨ì¼ RSS í”¼ë“œ ë¹„ë™ê¸° ìˆ˜ì§‘"""
        try:
            async with session.get(
                source['url'], 
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as response:
                content = await response.text()
                
                # feedparserëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ executorì—ì„œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                feed = await loop.run_in_executor(None, feedparser.parse, content)
                
                news_items = []
                for entry in feed.entries[:50]:  # ìµœëŒ€ 50ê°œë§Œ
                    try:
                        news_item = {
                            'title': entry.get('title', ''),
                            'link': entry.get('link', ''),
                            'summary': entry.get('summary', entry.get('description', ''))[:500],
                            'published': self._parse_date(entry),
                            'source': source['name'],
                            'raw_entry': entry
                        }
                        
                        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                        if news_item['title'] and news_item['link']:
                            news_items.append(news_item)
                            
                    except Exception as e:
                        logger.warning(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜ ({source['name']}): {e}")
                        continue
                
                logger.info(f"âœ… {source['name']}: {len(news_items)}ê±´ ìˆ˜ì§‘")
                return news_items
                
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {source['name']}")
            return []
        except Exception as e:
            logger.error(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨ ({source['name']}): {e}")
            return []
    
    async def collect_all(self) -> List[Dict]:
        """ëª¨ë“  RSS í”¼ë“œ ë¹„ë™ê¸° ìˆ˜ì§‘"""
        logger.info(f"ğŸ“¡ {len(self.rss_sources)}ê°œ RSS ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_feed(session, source) for source in self.rss_sources]
            results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ë³‘í•©
        all_news = []
        for news_list in results:
            all_news.extend(news_list)
        
        logger.info(f"ğŸ‰ ì´ {len(all_news)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_news
    
    def _parse_date(self, entry) -> str:
        """ë‚ ì§œ íŒŒì‹± (UTC â†’ KST ë³€í™˜)"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                # UTC ì‹œê°„ìœ¼ë¡œ íŒŒì‹±
                dt_utc = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                # í•œêµ­ ì‹œê°„(KST, UTC+9)ìœ¼ë¡œ ë³€í™˜
                dt_kst = dt_utc + timedelta(hours=9)
                return dt_kst.strftime('%Y-%m-%d %H:%M:%S')
            elif hasattr(entry, 'published'):
                return entry.published
        except:
            pass
        
        # í˜„ì¬ í•œêµ­ ì‹œê°„
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


def run_collector(rss_sources: List[Dict], timeout: int = 10) -> List[Dict]:
    """ë™ê¸° ì¸í„°í˜ì´ìŠ¤ (ë©”ì¸ì—ì„œ í˜¸ì¶œìš©)"""
    collector = RSSCollector(rss_sources, timeout)
    return asyncio.run(collector.collect_all())


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    test_sources = [
        {
            "name": "í…ŒìŠ¤íŠ¸_êµ¬ê¸€ë‰´ìŠ¤",
            "url": "https://news.google.com/rss/search?q=ì£¼ì‹&hl=ko&gl=KR&ceid=KR:ko",
            "enabled": True
        }
    ]
    
    news = run_collector(test_sources)
    print(f"\nìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news)}ê±´")
    if news:
        print("\nì²« ë²ˆì§¸ ë‰´ìŠ¤:")
        print(f"ì œëª©: {news[0]['title']}")
        print(f"ì¶œì²˜: {news[0]['source']}")

